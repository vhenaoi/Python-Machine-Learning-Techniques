{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmvz553B2zvxCxdTNpI+ea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Import the libraries that we are going to use in the code\n","import numpy as np  # Import NumPy for numerical operations and array manipulations\n","import pandas as pd  # Import pandas for data manipulation and analysis\n","import matplotlib.pyplot as plt  # Import Matplotlib for creating plots and visualizations"],"metadata":{"id":"SmixMdG2ty9E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation of the Metrics:**\n","\n","- **Accuracy**: Proportion of correct predictions out of the total number of predictions. It is calculated as:\n","\n","  $\n","  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n","  $\n","\n","  where \\( TP \\) is true positives, \\( TN \\) is true negatives, \\( FP \\) is false positives, and \\( FN \\) is false negatives.\n","\n","- **Sensitivity (or Recall)**: Proportion of true positives among the actual positives. It measures how well the model identifies positive instances. It is calculated as:\n","\n","  $\n","  \\text{Sensitivity} = \\frac{TP}{TP + FN}\n","  $\n","\n","- **Specificity**: Proportion of true negatives among the actual negatives. It measures how well the model identifies negative instances. It is calculated as:\n","\n","  $\n","  \\text{Specificity} = \\frac{TN}{TN + FP}\n","  $\n","\n","- **Precision**: Proportion of true positives among all positive predictions made by the model. It reflects the accuracy of the positive predictions. It is calculated as:\n","\n","  $\n","  \\text{Precision} = \\frac{TP}{TP + FP}\n","  $\n","\n","- **F1 Score**: Harmonic mean of precision and sensitivity (recall). It provides a single metric that balances precision and recall. It is calculated as:\n","\n","  $\n","  \\text{F1 Score} = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n","  $\n","\n","- **RMSLE (Root Mean Squared Logarithmic Error)**: Measures the mean squared error on the logarithm of the predicted values, typically used when predictions are in the form of counts or probabilities. It is calculated as:\n","\n","  $\n","  \\text{RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\log(\\hat{y}_i + 1) - \\log(y_i + 1))^2}\n","  $\n","\n","  where \\( \\hat{y}_i \\) are the predicted values and \\( y_i \\) are the actual values.\n","\n","- **Log Loss**: Measure of the performance of a classifier based on the probabilities it assigns to each class. It penalizes false classifications with a cost that is proportional to the confidence of the incorrect prediction. It is calculated as:\n","\n","  $\n","  \\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n","  $\n","\n","  where \\( \\hat{y}_i \\) are the predicted probabilities and \\( y_i \\) are the actual binary outcomes.\n","\n","- **MSE (Mean Squared Error)**: Average of the squared errors between predicted values and actual values. It is calculated as:\n","\n","  $\n","  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n","  $\n","\n","- **RMSE (Root Mean Squared Error)**: Square root of the MSE. It provides the standard deviation of the residuals (prediction errors) and is in the same units as the target variable. It is calculated as:\n","\n","  $\n","  \\text{RMSE} = \\sqrt{\\text{MSE}}\n","  $\n","\n","- **MAE (Mean Absolute Error)**: Average of the absolute errors between predicted values and actual values. It measures the average magnitude of the errors without considering their direction. It is calculated as:\n","\n","  $\n","  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i|\n","  $\n","\n","- **AUC (Area Under the Curve)**: Refers to the area under the Receiver Operating Characteristic (ROC) curve. It measures the model's ability to discriminate between positive and negative classes, with a value of 1 indicating perfect discrimination and 0.5 indicating no discrimination."],"metadata":{"id":"5VGltthMthO7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ul0ZLc40tY52"},"outputs":[],"source":["# Generate an array 't1_actual' of 20 random integers (either 0 or 1)\n","t1_actual = np.random.randint(2, size=20)\n","\n","# Generate an array 't1_predicted' based on 't1_actual'.\n","# Each element is 0 if the corresponding random number is greater than a threshold,\n","# otherwise, it copies the corresponding value from 't1_actual'.\n","# The threshold is determined by a random number scaled by 0.9 plus 0.05.\n","t1_predicted = np.abs(t1_actual * (np.random.random(size=20) > (np.random.random() * .9 + .05)).astype(int))\n","\n","# Print the actual values from 't1_actual', joining them into a comma-separated string\n","print(\"actual   \", \", \".join([str(i) for i in t1_actual]))\n","\n","# Print the predicted values from 't1_predicted', joining them into a comma-separated string\n","print(\"predicted\", \", \".join([str(i) for i in t1_predicted]))"]},{"cell_type":"markdown","source":["# **Accuracy**\n","\n","---\n","\n","* The function calculates accuracy by comparing true labels with predicted labels.\n","* It uses np.mean to compute the proportion of correctly predicted values."],"metadata":{"id":"AqOElFhQtp_S"}},{"cell_type":"code","source":["# Define a function 'accuracy' that calculates the accuracy of predictions.\n","# 'y_true' is the array of true labels, and 'y_pred' is the array of predicted labels.\n","def accuracy(y_true, y_pred):\n","    # Compute the accuracy as the mean of the comparison between 'y_true' and 'y_pred'.\n","    # The comparison 'y_true == y_pred' results in a boolean array where True (1) represents correct predictions and False (0) represents incorrect predictions.\n","    # np.mean calculates the average of these boolean values, which corresponds to the proportion of correct predictions.\n","    return np.mean(y_true == y_pred)"],"metadata":{"id":"TQ3NFLyVtsVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Sensitivity**\n","\n","---\n","*  True Positives (TP): Correctly predicted positive cases.\n","*  False Negatives (FN): Actual positive cases incorrectly predicted as negative.\n","*  Sensitivity: Measures the proportion of actual positives correctly identified."],"metadata":{"id":"dKc-d62Nt4Rz"}},{"cell_type":"code","source":["# Define a function 'sensitivity' to calculate the sensitivity (True Positive Rate) of predictions.\n","# 'y_true' is the array of true labels, and 'y_pred' is the array of predicted labels.\n","def sensitivity(y_true, y_pred):\n","    # Calculate True Positives (TP): the number of instances where both the true label and the predicted label are 1.\n","    tp = np.sum((y_true == 1) & (y_pred == 1))\n","\n","    # Calculate False Negatives (FN): the number of instances where the true label is 1 but the predicted label is 0.\n","    fn = np.sum((y_true == 1) & (y_pred == 0))\n","\n","    # Compute Sensitivity as TP divided by the sum of TP and FN.\n","    # Sensitivity (True Positive Rate) = TP / (TP + FN)\n","    # Ensure to handle the case where (TP + FN) is zero to avoid division by zero errors.\n","    # Return 0 if (TP + FN) is 0.\n","    return tp / (tp + fn) if (tp + fn) > 0 else 0"],"metadata":{"id":"dO-aml-bt6Nr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **RMSLE**\n","\n","---\n","* Logarithmic Transformation: np.log1p(x) computes log(1 + x), which helps to handle values near zero more effectively.\n","* RMSLE: Measures the average magnitude of the error in logarithmic terms, and is useful when dealing with exponential growth or data with a large range."],"metadata":{"id":"S9nMLY9NuHEU"}},{"cell_type":"code","source":["# Define a function 'rmsle' to compute the Root Mean Squared Logarithmic Error (RMSLE) between true and predicted values.\n","# 'y_true' is the array of true values, and 'y_pred' is the array of predicted values.\n","def rmsle(y_true, y_pred):\n","    # Compute the logarithmic difference between the predicted values and the true values.\n","    # np.log1p(x) computes the natural logarithm of (1 + x), which is more numerically stable for small values.\n","    log_diff = np.log1p(y_pred) - np.log1p(y_true)\n","\n","    # Calculate the mean of the squared logarithmic differences.\n","    # np.mean(log_diff ** 2) computes the average of squared log differences.\n","    # np.sqrt() takes the square root to return the Root Mean Squared Logarithmic Error.\n","    return np.sqrt(np.mean(log_diff ** 2))"],"metadata":{"id":"ZT-zuAVduJm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **logloss**\n","\n","---\n","* Epsilon: A small constant added to probabilities to avoid taking the logarithm of zero, which would be undefined and lead to numerical instability.\n","* Clipping: Adjusts probabilities to ensure they are within a valid range for logarithmic calculations.\n","* Log Loss Calculation: Measures the performance of a classification model where predictions are probabilities. It penalizes incorrect predictions more heavily the farther they are from the true label.\n"],"metadata":{"id":"GIoqPWkVuMOt"}},{"cell_type":"code","source":["# Define a function 'log_loss' to compute the Logarithmic Loss (Log Loss) between true labels and predicted probabilities.\n","# 'y_true' is the array of true binary labels, and 'y_pred_proba' is the array of predicted probabilities.\n","def log_loss(y_true, y_pred_proba):\n","    # Define a small constant 'epsilon' to avoid log(0), which would result in -inf or NaN values.\n","    epsilon = 1e-15\n","\n","    # Clip the predicted probabilities to ensure they are within the range [epsilon, 1 - epsilon].\n","    # This prevents taking the log of zero or one, which would result in numerical instability.\n","    y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n","\n","    # Compute the Log Loss.\n","    # - For each prediction, calculate the log loss as:\n","    #   - y_true * log(y_pred_proba) + (1 - y_true) * log(1 - y_pred_proba)\n","    # - Take the mean of these values and negate it to get the final Log Loss.\n","    return -np.mean(y_true * np.log(y_pred_proba) + (1 - y_true) * np.log(1 - y_pred_proba))"],"metadata":{"id":"xH7BvLd8uO0U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Specificity**\n","\n","---\n","\n","* True Negatives (TN): Number of instances where the model correctly predicts the negative class.\n","* False Positives (FP): Number of instances where the model incorrectly predicts the positive class when it is actually negative.\n","* Specificity: Measures the proportion of actual negatives that are correctly identified by the model. It is a useful metric when the cost of false positives is high."],"metadata":{"id":"gwJ92s1luRPm"}},{"cell_type":"code","source":["# Define a function 'specificity' to compute the Specificity of a binary classification model.\n","# 'y_true' is the array of true binary labels, and 'y_pred' is the array of predicted labels.\n","def specificity(y_true, y_pred):\n","    # Calculate the number of true negatives (TN):\n","    # TN: True Negative count where both true labels and predictions are 0.\n","    tn = np.sum((y_true == 0) & (y_pred == 0))\n","\n","    # Calculate the number of false positives (FP):\n","    # FP: False Positive count where true labels are 0 but predictions are 1.\n","    fp = np.sum((y_true == 0) & (y_pred == 1))\n","\n","    # Compute Specificity as the ratio of True Negatives to the sum of True Negatives and False Positives:\n","    # Specificity = TN / (TN + FP)\n","    # Return 0 if the denominator is 0 to avoid division by zero.\n","    return tn / (tn + fp) if (tn + fp) > 0 else 0"],"metadata":{"id":"ZChGNAgUuTu2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Precision**\n","\n","---\n","* True Positives (TP): Number of instances where the model correctly predicts the positive class.\n","* False Positives (FP): Number of instances where the model incorrectly predicts the positive class when it is actually negative.\n","* Precision: Measures the proportion of positive predictions that are actually correct. It is useful when the cost of false positives is high and you want to ensure that when a positive prediction is made, it is as accurate as possible."],"metadata":{"id":"FyXTsV6euWM-"}},{"cell_type":"code","source":["# Define a function 'precision' to compute the Precision of a binary classification model.\n","# 'y_true' is the array of true binary labels, and 'y_pred' is the array of predicted labels.\n","def precision(y_true, y_pred):\n","    # Calculate the number of true positives (TP):\n","    # TP: True Positive count where both true labels and predictions are 1.\n","    tp = np.sum((y_true == 1) & (y_pred == 1))\n","\n","    # Calculate the number of false positives (FP):\n","    # FP: False Positive count where true labels are 0 but predictions are 1.\n","    fp = np.sum((y_true == 0) & (y_pred == 1))\n","\n","    # Compute Precision as the ratio of True Positives to the sum of True Positives and False Positives:\n","    # Precision = TP / (TP + FP)\n","    # Return 0 if the denominator is 0 to avoid division by zero.\n","    return tp / (tp + fp) if (tp + fp) > 0 else 0"],"metadata":{"id":"xOI4dn4XuY6Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **F1_Score**\n","\n","---\n","* Precision: Proportion of positive predictions that are actually correct.\n","* Sensitivity (Recall): Proportion of actual positives that are correctly identified.\n","* F1 Score: Harmonic mean of Precision and Sensitivity. It balances Precision and Recall, providing a single metric that considers both false positives and false negatives. It is particularly useful when the class distribution is imbalanced or when both Precision and Recall are important."],"metadata":{"id":"mKtzrTjdubA4"}},{"cell_type":"code","source":["# Define a function 'f1_score' to compute the F1 Score of a binary classification model.\n","# 'y_true' is the array of true binary labels, and 'y_pred' is the array of predicted labels.\n","def f1_score(y_true, y_pred):\n","    # Calculate Precision using the previously defined 'precision' function.\n","    prec = precision(y_true, y_pred)\n","\n","    # Calculate Sensitivity (True Positive Rate) using the previously defined 'sensitivity' function.\n","    sens = sensitivity(y_true, y_pred)\n","\n","    # Compute the F1 Score as the harmonic mean of Precision and Sensitivity:\n","    # F1 Score = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)\n","    # Return 0 if the denominator is 0 to avoid division by zero.\n","    return 2 * (prec * sens) / (prec + sens) if (prec + sens) > 0 else 0"],"metadata":{"id":"sLblMDXjud_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **MSE**\n","\n","---\n","Mean Squared Error (MSE): A common metric used to measure the average squared difference between predicted and actual values. Lower values of MSE indicate better model performance, as it means the predicted values are closer to the true values. MSE penalizes larger errors more heavily than smaller errors because the differences are squared."],"metadata":{"id":"l4xuG6sVug2l"}},{"cell_type":"code","source":["# Define a function 'mse' to compute the Mean Squared Error (MSE) between true and predicted values.\n","# 'y_true' is the array of true values, and 'y_pred' is the array of predicted values.\n","def mse(y_true, y_pred):\n","    # Calculate the Mean Squared Error (MSE):\n","    # 1. Compute the squared difference between each pair of true and predicted values: (y_true - y_pred) ** 2\n","    # 2. Compute the mean of these squared differences: mean((y_true - y_pred) ** 2)\n","    return np.mean((y_true - y_pred) ** 2)"],"metadata":{"id":"LgBdatVFui7m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**RMSE**\n","---\n","\n","Root Mean Squared Error (RMSE): The square root of the Mean Squared Error\n","(MSE). It provides a measure of the average magnitude of the errors in the same units as the original data. RMSE is often preferred over MSE because it has the same units as the target variable, making it easier to interpret."],"metadata":{"id":"6NUp-_maul_n"}},{"cell_type":"code","source":["# Define a function 'rmse' to compute the Root Mean Squared Error (RMSE) between true and predicted values.\n","# 'y_true' is the array of true values, and 'y_pred' is the array of predicted values.\n","def rmse(y_true, y_pred):\n","    # Compute the Root Mean Squared Error (RMSE):\n","    # 1. Calculate the Mean Squared Error (MSE) by calling the 'mse' function.\n","    # 2. Take the square root of the MSE to get the RMSE.\n","    return np.sqrt(mse(y_true, y_pred))"],"metadata":{"id":"in8jgXVyun1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **MAE**\n","\n","---\n","Mean Absolute Error (MAE): It measures the average magnitude of the errors in a set of predictions, without considering their direction (i.e., positive or negative). MAE is the average of the absolute differences between the predicted and actual values. It is useful when you want a straightforward interpretation of the error magnitude in the same units as the data."],"metadata":{"id":"_VHl7NxxupoX"}},{"cell_type":"code","source":["def mae(y_true, y_pred):\n","    # Mean Absolute Error = mean(|y_true - y_pred|)\n","    return np.mean(np.abs(y_true - y_pred))"],"metadata":{"id":"BJ8zahQ5usuY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **AUC**\n","\n","---\n","* **Compute_roc_curve Function**:\n","\n","*Purpose*: Calculates True Positive Rate (TPR) and False Positive Rate (FPR) for various thresholds.\n","*Steps*:\n","1. *Thresholds*: Sort unique predicted probabilities.\n","2. *Metrics*: For each threshold, compute TPR (TP / (TP + FN)) and FPR (FP / (FP + TN)).\n","3. *Return*: TPR and FPR arrays for plotting the ROC curve.\n","\n","* **AUC Function**:\n","\n","*Purpose*: Computes the Area Under the Curve (AUC) from the ROC curve.\n","*Steps*:\n","1. *Get ROC Data*: Retrieve TPR and FPR from compute_roc_curve.\n","2. *Calculate AUC*: Use the trapezoidal rule to integrate TPR vs. FPR.\n","3. *Return*: AUC value representing classifier performance."],"metadata":{"id":"vufXqLNouuPJ"}},{"cell_type":"code","source":["def compute_roc_curve(y_true, y_pred_proba):\n","    # Sort unique predicted probabilities to create thresholds\n","    thresholds = np.sort(np.unique(y_pred_proba))\n","    tpr = []  # List to store True Positive Rates\n","    fpr = []  # List to store False Positive Rates\n","\n","    for threshold in thresholds:\n","        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n","        tp = np.sum((y_true == 1) & (y_pred_binary == 1))  # True Positives\n","        fp = np.sum((y_true == 0) & (y_pred_binary == 1))  # False Positives\n","        fn = np.sum((y_true == 1) & (y_pred_binary == 0))  # False Negatives\n","        tn = np.sum((y_true == 0) & (y_pred_binary == 0))  # True Negatives\n","\n","        tpr.append(tp / (tp + fn) if (tp + fn) > 0 else 0)  # True Positive Rate\n","        fpr.append(fp / (fp + tn) if (fp + tn) > 0 else 0)  # False Positive Rate\n","\n","    # Ensure FPR and TPR are sorted\n","    sorted_indices = np.argsort(fpr)\n","    fpr = np.array(fpr)[sorted_indices]\n","    tpr = np.array(tpr)[sorted_indices]\n","\n","    return fpr, tpr"],"metadata":{"id":"WqZzFHuOuxDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def auc(y_true, y_pred_proba):\n","    # Compute the ROC curve\n","    fpr, tpr = compute_roc_curve(y_true, y_pred_proba)\n","\n","    # Compute the AUC using trapezoidal rule\n","    auc_value = np.trapz(tpr, fpr)\n","\n","    return auc_value"],"metadata":{"id":"o8ok_mGMuzvK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Calculate metrics**"],"metadata":{"id":"moek7DvBu2JY"}},{"cell_type":"code","source":["# Calculate metrics\n","acc = accuracy(t1_actual, t1_predicted)\n","sens = sensitivity(t1_actual, t1_predicted)\n","spec = specificity(t1_actual, t1_predicted)\n","prec = precision(t1_actual, t1_predicted)\n","f1 = f1_score(t1_actual, t1_predicted)\n","rmsle_value = rmsle(t1_actual, t1_predicted)\n","log_loss_value = log_loss(t1_actual, t1_predicted + 1e-15)  # Adding small value to avoid log(0)\n","mse_value = mse(t1_actual, t1_predicted)\n","rmse_value = rmse(t1_actual, t1_predicted)\n","mae_value = mae(t1_actual, t1_predicted)\n","auc_value = auc(t1_actual, t1_predicted)\n","\n","# Print results\n","print(f\"Accuracy: {acc}\")\n","print(f\"Sensitivity: {sens}\")\n","print(f\"Specificity: {spec}\")\n","print(f\"Precision: {prec}\")\n","print(f\"F1 Score: {f1: .3f}\") #reduce the number of decimal\n","print(f\"RMSLE: {rmsle_value}\")\n","print(f\"Log Loss: {log_loss_value}\")\n","print(f\"MSE: {mse_value}\")\n","print(f\"RMSE: {rmse_value}\")\n","print(f\"MAE: {mae_value}\")\n","print(f\"AUC: {auc_value}\")"],"metadata":{"id":"E5G1RqDsu3ki"},"execution_count":null,"outputs":[]}]}